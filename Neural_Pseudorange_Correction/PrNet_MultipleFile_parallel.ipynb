{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard API\n",
    "import collections\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# User defined API\n",
    "from ReadingRawGnssDataset import readingRawGnssDataset\n",
    "from PrNet_parallel import MlpFeatureExtractor\n",
    "from PrNet_parallel import PrNet\n",
    "from train_PrNet_parallel import train_gnss_net\n",
    "from DataLoader_MultipleFile_Random_NoTime import GnssMultipleDataFileLoader\n",
    "from DataLoader_SingleFile_NoTime import GNSSSingleDataFileLoader\n",
    "from Evaluate_PrNet_parallel import evaluate_gnss_net\n",
    "\n",
    "# Set the global data type\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% *********************************** Parameter Settings ***********************************\n",
    "# A minibatch is organized in a tensor with size [batch_size * PRN_size * Input_size]\n",
    "# Input size of rural data\n",
    "input_size = 39\n",
    "\n",
    "# Input size of urban data\n",
    "# input_size = 55\n",
    "\n",
    "PRN_size = 32\n",
    "res_size = 1\n",
    "label_size = 3\n",
    "\n",
    "# %% *********************************** Reading training data in multiple data files ***********************************\n",
    "training_data_dir = \"../Data/RouteR/Training/\"\n",
    "# Get all files in the current directory\n",
    "training_data_files = os.listdir(training_data_dir)\n",
    "Xfeatures_list = []\n",
    "Reslabels_list = []\n",
    "ylabels_list = []\n",
    "for data_file in training_data_files:\n",
    "    # If the object is not a directory, we will open and read it\n",
    "    data_file_path = training_data_dir+'/'+data_file\n",
    "    if not os.path.isdir(data_file_path):\n",
    "        # Read one data file\n",
    "        data = pd.read_csv(data_file_path)\n",
    "        inputs, outputs = torch.tensor(data.iloc[:, 0:input_size].values), torch.tensor(data.iloc[:, 14:input_size].values)\n",
    "        # features, pseudorange residuals, labels\n",
    "        Xfeatures, Reslabels, ylabels = readingRawGnssDataset(inputs, outputs, input_size, res_size, label_size,\n",
    "                                                              PRN_size)  # a list of tensors\n",
    "        # Lists of input data files\n",
    "        Xfeatures_list.append(Xfeatures)\n",
    "        Reslabels_list.append(Reslabels)\n",
    "        ylabels_list.append(ylabels)\n",
    "\n",
    "sum_samples = 0\n",
    "for data_file in training_data_files:\n",
    "    # If the object is not a directory, we will open and read it\n",
    "    data_file_path = training_data_dir+'/'+data_file\n",
    "    if not os.path.isdir(data_file_path):\n",
    "        # Read one data file\n",
    "        data = pd.read_csv(data_file_path)\n",
    "        sum_samples = sum_samples+len(data.iloc[:,:].values)\n",
    "sum_samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  *********************************** PrNet ***********************************\n",
    "# Size of input features of Encoder's MLP\n",
    "# CN0, sinE, cosE, PRN, Wls_lon*3, Wls_lat*3, Unit_geometry_vector*3, heading*3\n",
    "input_size_debiasing = 16 \n",
    "\n",
    "# Number of hidden neurons on the hidden layer of the MLP\n",
    "#H\n",
    "num_hiddens_debiasing = 40\n",
    "\n",
    "# Number of layers of the MLP\n",
    "#L\n",
    "num_debiasing_layers = 20\n",
    "\n",
    "# Dropout probability\n",
    "dropout = 0\n",
    "extractor = MlpFeatureExtractor(input_size_debiasing, num_hiddens_debiasing, \n",
    "                                num_debiasing_layers, dropout=0)\n",
    "net = PrNet(extractor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training\"\"\"\n",
    "# batch size in rural areas\n",
    "batch_size = 256 \n",
    "\n",
    "# batch size in urban areas\n",
    "# batch_size = 32\n",
    "\n",
    "# Define data iterator\n",
    "data_iter = GnssMultipleDataFileLoader(Xfeatures_list, Reslabels_list, batch_size)\n",
    "\n",
    "num_epochs_seq = [100, 100, 100, 100, 100]\n",
    "num_iterations = 1\n",
    "lr = 0.01\n",
    "for num_epochs in num_epochs_seq:\n",
    "    optimizer = train_gnss_net(net, data_iter, lr, num_epochs, num_iterations, d2l.try_gpu())\n",
    "    lr = lr/10\n",
    "\n",
    "torch.save({\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, 'PrNet_Layer20_H40_heading_RouteR_500.tar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A minibatch is organized in a tensor with size [batch_size * PRN_size * Input_size]\n",
    "# Input size of rural data\n",
    "input_size = 39\n",
    "\n",
    "# Input size of urban data\n",
    "# input_size = 55\n",
    "\n",
    "PRN_size = 32\n",
    "res_size = 1\n",
    "label_size = 3\n",
    "\n",
    "# A minibatch is organized in a tensor with size [batch_size * MovingWindowSize * PRN_size * Input_size]\n",
    "batch_size_eval = 1\n",
    "\n",
    "# %% *********************************** Reading testing data  ***********************************\n",
    "data_file_eval = \"../Data/RouteR/Testing/SvPVT3D_Error_label_dynamic_2020-05-14-US-MTV-1.csv\"\n",
    "data_eval = pd.read_csv(data_file_eval)\n",
    "\n",
    "inputs_eval, outputs_eval = torch.tensor(data_eval.iloc[:, 0:input_size].values), torch.tensor(data_eval.iloc[:, 14:input_size].values)\n",
    "\n",
    "# features, pseudorange residuals, labels\n",
    "x_features_eval, res_labels_eval, y_labels_eval = readingRawGnssDataset(inputs_eval, outputs_eval, input_size, res_size,\n",
    "                                                                        label_size, PRN_size)  # a list of tensors\n",
    "plt.figure()\n",
    "plt.plot(torch.stack([sum(y[y[:,0]!=0, 0])/sum(y[:,0]!=0) for y in y_labels_eval]),\n",
    "         torch.stack([sum(y[y[:,0]!=0, 1])/sum(y[:,0]!=0) for y in y_labels_eval]), 'v')         \n",
    "plt.show()\n",
    "\n",
    "# Define the data iterator for evaluation\n",
    "data_iter_eval = GNSSSingleDataFileLoader(x_features_eval, res_labels_eval, batch_size_eval)\n",
    "\n",
    "\n",
    "# %%  *********************************** PrNet ***********************************\n",
    "# Size of input features of Encoder's MLP\n",
    "# CN0, sinE, cosE, PRN, Wls_lon*3, Wls_lat*3, Unit_geometry_vector*3, heading*3\n",
    "input_size_debiasing = 16 \n",
    "# 16 \n",
    "\n",
    "# Number of hidden neurons on the hidden layer of the Encoder's MLP\n",
    "num_hiddens_debiasing = 40 \n",
    "\n",
    "# Number of layers of the Encoder's MLP\n",
    "num_debiasing_layers = 20 \n",
    "\n",
    "extractor = MlpFeatureExtractor(input_size_debiasing, num_hiddens_debiasing, num_debiasing_layers, dropout = 0)\n",
    "model_eval = PrNet(extractor)\n",
    "\n",
    "# The number of trainable parameters\n",
    "pytorch_total_params = sum(p.numel() for p in model_eval.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n",
    "\n",
    "# Load the trained PrNet.tar\n",
    "checkpoint = torch.load('Weights/RouteR/PrNet_Layer20_H40_heading_RouteR_500.tar')\n",
    "model_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluation\n",
    "model_eval.eval()\n",
    "prm_bias = evaluate_gnss_net(model_eval, data_iter_eval, batch_size_eval, d2l.try_gpu())\n",
    "\n",
    "# Write debiased results to a .csv file\n",
    "prm_bias_np = prm_bias.cpu().detach().numpy()\n",
    "prm_bias_np_df = pd.DataFrame(prm_bias_np)\n",
    "prm_bias_np_df.to_csv('PrM_Bias_2020-05-14-US-MTV-1.csv',header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e64507d9cd60dea4fee31d79d230a788ea1dbfb515ea6a5126cb28a0a3a0948"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
